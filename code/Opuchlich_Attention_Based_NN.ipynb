{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from feature_extraction import TextFeatureCreator\n",
    "\n",
    "def roundup(x):\n",
    "    return int(math.ceil(x / 10.0)) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#zip_ref = zipfile.ZipFile(\"clean_newsela_article_files.zip\", 'r')\n",
    "#zip_ref.extractall(\"clean_newsela\")\n",
    "#zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Based Neural Networks\n",
    "\n",
    "## Import Section\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21490"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import training and test set\n",
    "df = pd.read_csv(\"/data/ts_cost_function/newsela_article_feature_scores.csv\",sep=\";\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Scorer\n",
    "ts = TextFeatureCreator(\"/data/ts_cost_function/LSATtexts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.60611227e+00, 3.10202027e+01, 3.18078798e-01, 4.72160037e+01,\n",
       "       4.57929050e-01, 7.43301324e+00, 4.42320513e+00, 2.82743677e+00,\n",
       "       1.52766676e+00, 1.39530366e+00, 1.87133128e+00, 1.08050328e+00,\n",
       "       6.62886357e-01, 1.08484166e+00, 1.20740261e-01, 3.11994654e-01,\n",
       "       1.65647696e-02, 1.52961211e+00, 1.98660670e-01, 3.44356431e+00,\n",
       "       3.82638558e+00])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.feature_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset\n",
    "\n",
    "### Number of different Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/data/ts_cost_function/clean_newsela/\"\n",
    "#test_path = test_path[:5] + \"clean_\" + test_path[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_file_text(df):\n",
    "    for ind,row in df.iterrows():\n",
    "        if row[\"path\"] != 'data/LSATtexts.txt':\n",
    "            yield get_file_text(row,tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_text(row,tokenize=True):\n",
    "    path = row[\"path\"].split(\"/\")[-1]\n",
    "    path= \"/data/ts_cost_function/clean_newsela/\"+path\n",
    "    #if row[\"dataset\"] == \"newsela\":\n",
    "    #    path = row[\"path\"][:5] + \"clean_\" + row[\"path\"][5:]\n",
    "    with codecs.open(path, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        if tokenize:\n",
    "            text = [[word.text.lower() for word in nlp(sent)] for sent in text.split(\".\")]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eng_words(eng_words_path=\"data/20k_words.txt\"):\n",
    "\n",
    "    with open(eng_words_path,\"r\") as f:\n",
    "        data = f.read()\n",
    "        words = data.split(\"\\n\")\n",
    "        english_df = pd.DataFrame(data=words,columns=[\"word\"])\n",
    "        english_df[\"index\"] = range(0,len(english_df))\n",
    "    return english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [get_file_text(row) for ind,row in tqdm(df.iterrows()) if row[\"path\"] != 'data/LSATtexts.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_set = set()\n",
    "counter = Counter()\n",
    "for ind,row in tqdm(df.iterrows()):\n",
    "    lem_text = [token.lemma_ for token in nlp(get_file_text(row))]\n",
    "    counter.update(lem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/newsela_word_set.txt\",\"w\") as f:\n",
    "    f.write(\",\".join(word_set))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15419"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_array = np.array(list(word_set))\n",
    "english_df = load_eng_words()\n",
    "len(word_set.intersection(english_df[\"word\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Sentences and Words per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stats(df):\n",
    "    sents = []\n",
    "    words = []\n",
    "    removed = []\n",
    "    for ind,text in enumerate(yield_file_text(df)):\n",
    "        sent_split = text.split(\".\")\n",
    "        sents.append(len(sent_split))\n",
    "        words.append([word_tokenize(sent) for sent in sent_split])\n",
    "        if text.find(\"This article has been removed\") >= 0:\n",
    "            removed.append(ind)\n",
    "    \n",
    "    print(\"Max #Sents: {}\".format(np.max(sents)))\n",
    "    print(\"Min #Sents: {}\".format(np.min(sents)))\n",
    "    print(\"Std #Sents: {}\".format(np.std(sents)))\n",
    "    plt.boxplot(sents)\n",
    "    plt.show()\n",
    "    \n",
    "    return sents,words,removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max #Sents: 224\n",
      "Min #Sents: 14\n",
      "Std #Sents: 14.115212574729066\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAD5JJREFUeJzt3V9sVGd6x/Hf48HYwQ6NDS5K4lAqEVeTWupmNVpFwhehtCXkgqQ30RqpoHoElRosKlWiKXORVJVRhNVWrFFDYtlKkNppI7Wr5CLaNApTRQO7bU21iiBuCdomisH8SUBsmMSWsZ9ecGDtZMx4Zuwcz8v3I1kzfufYfiKRL4dzjueYuwsAEK66uAcAACwtQg8AgSP0ABA4Qg8AgSP0ABA4Qg8AgSP0ABA4Qg8AgSP0ABC4FXEPIElr1671DRs2xD0GANSUU6dOfe7ubaW2Wxah37Bhg0ZGRuIeAwBqipl9upDtOHQDAIEj9AAQOEIPAIEj9AAQOEIPAIEj9MA8stmsOjs7lUgk1NnZqWw2G/dIQEWWxeWVwHKTzWaVyWQ0NDSkrq4u5fN5pdNpSVJ3d3fM0wHlseVwK8FUKuVcR4/lpLOzUwMDA9q8efOdtVwup97eXp0+fTrGyYBfMbNT7p4quR2hB74tkUhoYmJC9fX1d9ampqbU2Nio6enpGCcDfmWhoecYPVBEMplUPp+fs5bP55VMJmOaCKgcoQeKyGQySqfTyuVympqaUi6XUzqdViaTiXs0oGycjAWKuH3Ctbe3V6Ojo0omk+rr6+NELGoSx+gBoEZxjB4AIInQA0DwCD0ABI7QA0DgCD0ABI7QA0DgCD0ABI7QA0DgCD0ABI7QA0DgCD0ABI7QA0DgCD0ABI7QA0DgSobezB4xs5yZfWRmZ8xsX7TeambvmdnH0WNLtG5m9iMzO2dmH5rZ95f6PwIAML+F7NHflPTn7v6YpCckPW9mj0l6QdL77v6opPejzyVpm6RHo489kl5Z9KkBAAtWMvTuPu7u/x09/1LSqKSHJT0j6Y1oszckPRs9f0bSMb/lZ5IeMLMHF31yYIlls1l1dnYqkUios7NT2Ww27pGAipR1K0Ez2yDpcUn/IWmdu49HL12UtC56/rCkz2Z92Vi0Ni6gRmSzWWUyGQ0NDamrq0v5fF7pdFqSuJ0gas6CT8aaWbOkf5H0Z+7+y9mv+a37EZZ1T0Iz22NmI2Y2cuXKlXK+FFhyfX19Ghoa0ubNm1VfX6/NmzdraGhIfX19cY8GlG1BoTezet2K/D+4+79Gy5duH5KJHi9H6+clPTLry9ujtTnc/TV3T7l7qq2trdL5gSUxOjqqrq6uOWtdXV0aHR2NaSKgcgu56sYkDUkadfe/nfXS25J2Rc93SXpr1vrO6OqbJyRdn3WIB6gJyWRS+Xx+zlo+n1cymYxpIqByC9mj3yTpjyT9rpn9PPp4WtLLkn7fzD6W9HvR55L0jqRfSDonaVDSny7+2MDSymQySqfTyuVympqaUi6XUzqdViaTiXs0oGwlT8a6e16SzfPyliLbu6Tnq5wLiNXtE669vb0aHR1VMplUX18fJ2JRk+xWl+OVSqV8ZGQk7jEAoKaY2Sl3T5XajrdAAIDAEXoACByhB4DAEXoACByhB4DAEXoACByhB4DAEXoACByhB4DAEXoACByhB4DAEXpgHtxKEKEg9EAR2WxW+/btU6FQkLurUCho3759xB41idADRezfv1+JRELDw8OanJzU8PCwEomE9u/fH/doQNkIPVDE2NiYjh07NueesceOHdPY2FjcowFlI/TAPI4fPz7nGP3x48fjHgmoCKEHimhtbVV/f796enr05ZdfqqenR/39/WptbY17NKBshB4oYtWqVWpubtbAwIDuv/9+DQwMqLm5WatWrYp7NKBshB4o4sKFCxoYGFBTU5MkqampSQMDA7pw4ULMkwHlK3lzcOBelEwm1d7ertOnT99Zy+VySiaTMU4FVIbQA0VkMhk9++yz+vrrrzU1NaX6+nrdd999Onr0aNyjAWXj0A1QxMmTJ3Xjxg2tWbNGdXV1WrNmjW7cuKGTJ0/GPRpQNkIPFDE4OKj+/n6Nj49renpa4+Pj6u/v1+DgYNyjAWUj9EARk5OTOnv2rBobG2Vmamxs1NmzZzU5ORn3aEDZCD1QRF1dnQYHB3Xw4EEVCgUdPHhQg4ODqqvjfxnUHv7UAkWYmdx9zpq7y8ximgioHKEHipientbu3bt14MABNTU16cCBA9q9e7emp6fjHg0oG6EHimhoaNCZM2fmrJ05c0YNDQ0xTQRUjtADRXR0dOjEiRPaunWrrly5oq1bt+rEiRPq6OiIezSgbPzCFFDE2bNntWnTJr377rtqa2tTQ0ODNm3apJGRkbhHA8rGHj1QxOTkpNLptDZu3Ki6ujpt3LhR6XSayytRkwg9UMSKFSvU29urQqEgSSoUCurt7dWKFfwjGLWH0ANFNDQ0qFAoaNu2bbp69aq2bdumQqHAyVjUJEIPFFEoFLR9+3YNDw/rgQce0PDwsLZv335nDx+oJYQemMfevXs1MTEhd9fExIT27t0b90hARQg9UER7e7t27typXC6nqakp5XI57dy5U+3t7XGPBpSN0ANFHDp0SNPT0+rp6VFDQ4N6eno0PT2tQ4cOxT0aULaSoTezYTO7bGanZ629ZGbnzezn0cfTs177SzM7Z2b/a2Zbl2pwYCl1d3fr8OHDampqkpmpqalJhw8fVnd3d9yjAWVbyB7965KeKrL+d+7+vejjHUkys8ck/VDSb0df8/dmllisYQEA5St5UbC7f2BmGxb4/Z6R9E/uPinp/8zsnKQfSPppxRMCMchms8pkMhoaGlJXV5fy+bzS6bQksVePmlPNMfq9ZvZhdGinJVp7WNJns7YZi9aAmtLX16cdO3aot7dXjY2N6u3t1Y4dO9TX1xf3aEDZKv01v1ck/bUkjx7/RlJPOd/AzPZI2iNJ69evr3AMYGl89NFHunTpkpqbmyXduq7+1Vdf1RdffBHzZED5Ktqjd/dL7j7t7jOSBnXr8IwknZf0yKxN26O1Yt/jNXdPuXuqra2tkjGAJZNIJDQzM6Ph4WFNTExoeHhYMzMzSiQ45YTaU1HozezBWZ/+oaTbV+S8LemHZtZgZr8p6VFJ/1ndiMB37+bNm1q5cuWctZUrV+rmzZsxTQRUruShGzPLSnpS0lozG5P0oqQnzex7unXo5hNJfyJJ7n7GzN6U9JGkm5Ked3duyYOa9NBDD2nLli13biH4+OOP6+LFi3GPBZTNvnlfzDikUinnfb6xnDQ3N6tQKKiurk4zMzN3HpuamnTjxo24xwMkSWZ2yt1TpbbjN2OBIr766itJ0szMzJzH2+tALSH0QBHz/Ut3OfwLGCgXoQfuoqWlZc4jUIsIPVBCXR3/m6C28ScYuIvr169rZmZG169fj3sUoGKEHriLb56MBWoRoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAgcoQeAwBF6AAhcydCb2bCZXTaz07PWWs3sPTP7OHpsidbNzH5kZufM7EMz+/5SDg8AKG0he/SvS3rqG2svSHrf3R+V9H70uSRtk/Ro9LFH0iuLMyYAoFIlQ+/uH0i6+o3lZyS9ET1/Q9Kzs9aP+S0/k/SAmT24WMMCAMpX6TH6de4+Hj2/KGld9PxhSZ/N2m4sWgMAxKTqk7Hu7pK83K8zsz1mNmJmI1euXKl2DADAPCoN/aXbh2Six8vR+nlJj8zarj1a+xZ3f83dU+6eamtrq3AMAEAplYb+bUm7oue7JL01a31ndPXNE5KuzzrEAwCIwYpSG5hZVtKTktaa2ZikFyW9LOlNM0tL+lTSc9Hm70h6WtI5SV9J+uMlmBkAUIaSoXf37nle2lJkW5f0fLVDAQAWD78ZCwCBI/QAEDhCDwCBI/QAEDhCDwCBI/QAEDhCDwCBI/QAEDhCDwCBI/QAEDhCDwCBI/QAELiSb2oGhMTMvpPvcev9/YDlgdDjnrLQAN8t5kQctYZDN0AR88WcyKMWsUcPzON21M2MwKOmsUcPAIEj9AAQOEIPAIEj9AAQOEIPAIEj9AAQOEIPAIEj9AAQOEIPAIEj9AAQOEIPAIEj9AAQOEIPAIEj9AAQOEIPAIEj9AAQOEIPAIHjDlOoWa2trbp27dp38rMW46bid9PS0qKrV68u6c/AvYvQo2Zdu3YtmFv8LfVfJLi3cegGAAJH6AEgcIQeAAJX1TF6M/tE0peSpiXddPeUmbVK+mdJGyR9Iuk5d/9uzpgBAL5lMU7Gbnb3z2d9/oKk9939ZTN7Ifr8Lxbh5wBz+IurpZd+Le4xFoW/uDruERCwpbjq5hlJT0bP35D07yL0WAL2V78M6qobfynuKRCqao/Ru6R/M7NTZrYnWlvn7uPR84uS1lX5MwAAVah2j77L3c+b2a9Les/M/mf2i+7uZlZ0lyv6i2GPJK1fv77KMQAA86lqj97dz0ePlyX9WNIPJF0yswclKXq8PM/XvubuKXdPtbW1VTMGAOAuKg69mTWZ2f23n0v6A0mnJb0taVe02S5Jb1U7JACgctUculkn6cfRr26vkPSP7v4TM/svSW+aWVrSp5Keq35MAEClKg69u/9C0u8UWf9C0pZqhgIALB7e1Aw1LZQ3A2tpaYl7BASM0KNmfVfX0JtZMNfr497Ee90AQOAIPQAEjtADQOAIPQAEjtADQOAIPQAEjtADQOAIPQAEjtADQOAIPQAEjtADQOAIPQAEjtADQOAIPQAEjtADQOAIPQAEjhuPAPOYffeq28+5AQlqEXv0QBHz3aIwlFsX4t7CHj3uKYsR6oV8D/b8sZwQetxTFhrgu8WciKPWcOgGAAJH6AEgcIQeAAJH6AEgcIQeAAJH6AEgcIQeAAJH6AEgcIQeKKK1tbWsdWA5I/RAEUeOHNHq1atVX18vSaqvr9fq1at15MiRmCcDykfogSK6u7t19OhRdXR0qK6uTh0dHTp69Ki6u7vjHg0omy2H9+1IpVI+MjIS9xgAUFPM7JS7p0ptxx49AASO0ANA4Ag9AASO0ANA4Ag9AARuWVx1Y2ZXJH0a9xzAPNZK+jzuIYAifsPd20pttCxCDyxnZjaykEvYgOWKQzcAEDhCDwCBI/RAaa/FPQBQDY7RA0Dg2KMHgMARemAeZjZsZpfN7HTcswDVIPTA/F6X9FTcQwDVIvTAPNz9A0lX454DqBahB4DAEXoACByhB4DAEXoACByhB+ZhZllJP5X0W2Y2ZmbpuGcCKsFvxgJA4NijB4DAEXoACByhB4DAEXoACByhB4DAEXoACByhB4DAEXoACNz/A6Q0CB5YSLcJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents,words,removed = sentence_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array([np.array([len(sent) for sent in sents]) for sents in removed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max Word Count\n",
    "np.max([np.max(article) for article in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.720281280318863"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean Word Count\n",
    "np.mean([sent for art in words for sent in art])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with codecs.open(\"data/clean_newsela/3d-print-hand-violin-22566-1320.txt\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    example = f.read()\n",
    "    example = [[token.text.lower() for token in nlp(prefix_whitespace.sub(\"\",sent))] for sent in example.split(\".\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_whitespace = re.compile(\"^ +\")\n",
    "\n",
    "class WordTrainer(object):\n",
    "    def __init__(self, dir_name):\n",
    "        self.dir_name = dir_name\n",
    "    def __iter__(self):\n",
    "        for idx,file_name in enumerate(os.listdir(self.dir_name)):\n",
    "            for idxx,text in enumerate(\n",
    "                codecs.open(\n",
    "                    os.path.join(self.dir_name, file_name),'r',encoding='utf-8', errors='ignore')):\n",
    "                \n",
    "                sents = [[token.text.lower() \n",
    "                          for token in nlp(prefix_whitespace.sub(\"\",sent))] \n",
    "                         for sent in text.split(\".\")]\n",
    "                for words in sents:\n",
    "                    yield words\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_details = WordTrainer('/data/ts_cost_function/clean_newsela/')\n",
    "word_vector_model = Word2Vec(patient_details, size=100, window=4, min_count=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_model.save(\"/data/ts_cost_function/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_model = Word2Vec.load(\"/data/ts_cost_function/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use predefined word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_token_vectors(df,word2vec_model):\n",
    "    for ind,row in df.iterrows():\n",
    "        if row[\"path\"] != 'data/LSATtexts.txt':\n",
    "            tok_text =  get_file_text(row,tokenize=True)\n",
    "            yield vetorize_tokens(tok_text,word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorize_tokens(tok_text,word2vec_model):\n",
    "    vector_text = []\n",
    "    for sent in tok_text:\n",
    "        vector_sent = []\n",
    "        for token in sent:\n",
    "            try:\n",
    "                vector_sent.append(word2vec_model.wv[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vector_text.append(vector_sent)\n",
    "    return vector_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_emb_fn(df,word2vec_model, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((yield_token_vectors(df,word2vec_model), df[\"regression_score\"]))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'generator'> to Tensor. Contents: <generator object yield_token_vectors at 0x7f9aad9f74c0>. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     66\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 67\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <generator object yield_token_vectors at 0x7f9aad9f74c0>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-005173a9f4e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_word_emb_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_vector_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-225c3be535a0>\u001b[0m in \u001b[0;36mtrain_word_emb_fn\u001b[0;34m(df, word2vec_model, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"An input function for training\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Convert the inputs to a Dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myield_token_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"regression_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Shuffle, repeat, and batch the examples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1028\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1029\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m       ])\n\u001b[1;32m   1032\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1028\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1029\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m       ])\n\u001b[1;32m   1032\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1012\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                          as_ref=False):\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    519\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    520\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'generator'> to Tensor. Contents: <generator object yield_token_vectors at 0x7f9aad9f74c0>. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "train_word_emb_fn(df,word_vector_model,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_word_emb_fn(features, labels=None, batch_size=None):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert inputs to a tf.dataset object.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + Sentence RNN + Text RNN\n",
    "\n",
    "The sentence RNN will output a hidden state for each sentence. Afterwards each hidden state will be used as a input for another RNN on text level. The final output is than the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_emb_cnn_rnn_fn(features, labels, mode):\n",
    "    \n",
    "    input_layer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + Conv + RNN\n",
    "\n",
    "Taghipour, Kaveh, and Hwee Tou Ng. \"A neural approach to automated essay scoring.\" Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](A_Neural_Approach_to_Automated_Essay_Scoring.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_emb_cnn_rnn_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # features = words\n",
    "    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    #conv1 = tf.layers.conv2d(\n",
    "    #  inputs=input_layer,\n",
    "    #  filters=32,\n",
    "    #  kernel_size=[3, 3],\n",
    "    #  padding=\"same\",\n",
    "    #  activation=tf.nn.relu)\n",
    "    \n",
    "    conv1_layer = tf.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[3, 3],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        name=\"conv1_layer\"\n",
    "    )\n",
    "    conv1 = conv1_layer.apply(input_layer)\n",
    "    \n",
    "    #weights = conv1_layer.trainable_weights\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": softmax\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimator\n",
    "\n",
    "## 1D Conv Layer for Text Embedding\n",
    "\n",
    "\n",
    "## Recurrent Layer with each Conv Layer Output as Input\n",
    "## Output: Score\n",
    "\n",
    "## Mean over time of all RNN scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"tmp/mnist_convnet_model\")\n",
    "\n",
    "\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=2000)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier.train(\n",
    "    input_fn=lambda:train_input_fn(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        32),\n",
    "    steps=2000,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = mnist_classifier.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(X_test,y_test,len(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
