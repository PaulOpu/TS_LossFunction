{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Features from given Text\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_data\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pyphen\n",
    "dic = pyphen.Pyphen(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_pos_tagging(text):\n",
    "    \n",
    "    tok_text = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    "    tok_pos_text = list(zip(*[list(zip(*nltk.pos_tag(sent))) for sent in tok_text]))\n",
    "    \n",
    "    return tok_pos_text\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize_text(tok_text,pos_text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_text = [[lemmatizer.lemmatize(word,get_wordnet_pos(tag)) for word,tag in zip(*[sent,tags])]\n",
    "                for sent,tags in zip(*[tok_text,pos_text])]\n",
    "    return lem_text\n",
    "\n",
    "def count_sent_word_length(tok_text):\n",
    "    n_sent = len(tok_text)\n",
    "    sent_len = [len(words) for words in tok_text]\n",
    "    n_words = sum(sent_len)\n",
    "    word_len = [len(word) for words in tok_text for word in words]\n",
    "    \n",
    "    mean_word_len = np.mean(word_len)\n",
    "    mean_sent_len = np.mean(sent_len)\n",
    "    return mean_word_len,mean_sent_len\n",
    "    \n",
    "def load_basic_eng(basic_eng_path=\"data/basic_english.txt\"):\n",
    "\n",
    "    with open(basic_eng_path,\"r\") as f:\n",
    "        data = f.read()\n",
    "        words = data.split(\" , \")\n",
    "        basic_df = pd.DataFrame(data=words,columns=[\"word\"])\n",
    "        basic_df[\"index\"] = range(0,len(basic_df))\n",
    "    return basic_df\n",
    "\n",
    "def load_eng_words(eng_words_path=\"data/20k_words.txt\"):\n",
    "\n",
    "    with open(eng_words_path,\"r\") as f:\n",
    "        data = f.read()\n",
    "        words = data.split(\"\\n\")\n",
    "        english_df = pd.DataFrame(data=words,columns=[\"word\"])\n",
    "        english_df[\"index\"] = range(0,len(english_df))\n",
    "    return english_df\n",
    "\n",
    "def calc_basic_eng_ratio(text,basic_df):\n",
    "    text_set = set(np.concatenate(text))\n",
    "    return len(text_set.intersection(\n",
    "        basic_df[\"word\"].values))/len(text_set)\n",
    "\n",
    "\n",
    "def calc_syllables_count(tok_text):\n",
    "    return len(\n",
    "        np.concatenate(\n",
    "            [dic.inserted(word).split(\"-\") for sent in tok_text for word in sent]\n",
    "        )\n",
    "    )/len(tok_text)\n",
    "\n",
    "def TTR(pos_text): \n",
    "    words = {} \n",
    "    num_words = 0 \n",
    "    for line in pos_text: \n",
    "        for word in line: \n",
    "            num_words += 1 \n",
    "            if word in words: \n",
    "                words[word] += 1 \n",
    "            else: \n",
    "                words[word] = 1 \n",
    "    return len(words) / num_words \n",
    "\n",
    "\n",
    "def pos_token_count(pos_text,tag_symb):\n",
    "    #Token Count for first letter of Tokens\n",
    "    \n",
    "    return np.mean([len([tag for tag in sent if tag.startswith(tag_symb)]) for sent in pos_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features_V1(text):\n",
    "    #tokenize, pos tagging, lemmatize\n",
    "    tok_text,pos_text = tok_pos_tagging(text)\n",
    "    lem_text = lemmatize_text(tok_text,pos_text)\n",
    "    \n",
    "    #Mean Word Len, Mean Sent Len\n",
    "    mean_word_len,mean_sent_len = count_sent_word_length(tok_text)\n",
    "    \n",
    "    #Load English Vocabulary\n",
    "    basic_df = load_basic_eng()\n",
    "    english_df = load_eng_words()\n",
    "    non_basic_df = english_df.loc[\n",
    "        ~english_df[\"word\"].isin(basic_df[\"word\"])]\n",
    "    \n",
    "    #Basic English Ratio\n",
    "    basic_eng_ratio = calc_basic_eng_ratio(lem_text,basic_df)\n",
    "    \n",
    "    #Syllables per Sentence Ratio\n",
    "    syll_sent_ratio = calc_syllables_count(tok_text)\n",
    "    \n",
    "    #TTR\n",
    "    ttr_ratio = TTR(pos_text)\n",
    "    \n",
    "    #POS Token per Sentence\n",
    "    #Noun, Verb, Adjective, Adverb, Pronoun\n",
    "    token_count = []\n",
    "    for tag in [\"N\",\"VB\",\"J\",\"RB\",\"PRP\",\",\"]:\n",
    "        token_count += [pos_token_count(pos_text,tag)]\n",
    "    \n",
    "    #tok_text,pos_text,lem_text\n",
    "    return mean_word_len,mean_sent_len,basic_eng_ratio,syll_sent_ratio,ttr_ratio,token_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.35, 10.0, 0.23529411764705882, 15.0, 0.75, [1.5, 2.0, 0.5, 1.0, 0.5, 1.0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_features_V1(\"Nevertheless, this exercise is pretty difficult. I will drive to Barcelona, which is located in Spain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
